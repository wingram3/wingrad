{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26e1526-a3be-4781-9a38-3bea2c214afb",
   "metadata": {},
   "source": [
    "This model obviously has poor performance on the mnist dataset. It has no hyperparamters other than learning rate and batch size. The purpose of this demo is just to show that you can create neural nets that will lower a loss function using the Tensor, Layer, and MLP classes included in wingrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a4d606-11f7-4ca5-a958-064984cde9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wingrad.net import MLP\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eea9f275-1647-4a35-90b2-b995f492fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65030576-20b2-4ee7-a684-3f283be62495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot function, zeros are made into -1 to accomodate tanh function\n",
    "def one_hot(Y, num_classes):\n",
    "    oh = np.eye(num_classes)[Y.reshape(-1)]\n",
    "    oh[oh == 0] = -1\n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259863fe-b554-4069-92e7-98f03de2f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten, normalize, and reshape data for model\n",
    "train_x = np.asarray(train_x.reshape(train_x.shape[0], -1) / 255).T    \n",
    "test_x = np.asarray(test_x.reshape(test_x.shape[0], -1) / 255).T       \n",
    "train_y = one_hot(train_y, num_classes=10).T                           \n",
    "test_y = one_hot(test_y, num_classes=10).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211b52f8-a5a5-4b76-a47c-e057c51cef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = MLP(784, [16, 16, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657dff4d-c90a-4f17-9bf4-878bb9d78ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "\n",
    "    # handle minibatches\n",
    "    if batch_size is None:\n",
    "        Xb, yb = train_x, train_y \n",
    "    else:\n",
    "        batch = np.random.permutation(train_x.shape[1])[:batch_size]\n",
    "        Xb, yb = train_x[:, batch], train_y[:, batch]\n",
    "\n",
    "    # lists of individual training examples. len of the lists equal to batch_size.\n",
    "    inputs = [Xb[:, i].reshape(784, 1) for i in range(Xb.shape[1])]\n",
    "    labels = [yb[:, i].reshape(10, 1) for i in range(yb.shape[1])]\n",
    "\n",
    "    # forward pass to get predictions\n",
    "    pred = [model(x) for x in inputs]\n",
    "\n",
    "    # get the losses\n",
    "    loss = sum([(yout.sum(axis=0) - ygt.sum(axis=0))**2 for ygt, yout in zip(labels, pred)])\n",
    "\n",
    "    # accuracy percentage\n",
    "    accuracy = [(yi > 0) == (predi.data > 0) for yi, predi in zip(labels, pred)]\n",
    "\n",
    "    return loss, sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecf53bb5-843a-4a63-b6ff-8832d8971bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 37488.39972, accuracy: 46.875%\n",
      "step 50, loss: 37.98212, accuracy: 91.992%\n",
      "step 100, loss: 5.60114, accuracy: 88.867%\n",
      "step 150, loss: 10.03050, accuracy: 91.797%\n",
      "step 200, loss: 3.57794, accuracy: 91.992%\n",
      "step 250, loss: 1.20181, accuracy: 88.477%\n",
      "step 300, loss: 2.47125, accuracy: 90.039%\n",
      "step 350, loss: 1.67993, accuracy: 89.844%\n",
      "step 400, loss: 0.40910, accuracy: 90.820%\n",
      "step 450, loss: 0.33954, accuracy: 91.797%\n",
      "step 499, loss: 0.53495, accuracy: 90.820%\n"
     ]
    }
   ],
   "source": [
    "for k in range(500):\n",
    "\n",
    "    epochs = 500\n",
    "\n",
    "    # forward pass\n",
    "    total_loss, acc = loss(batch_size=512)\n",
    "\n",
    "    # zero grad\n",
    "    model.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    total_loss.backward()\n",
    "\n",
    "    # update parameters using adaptive moment estimation (Adam)\n",
    "    for p in model.parameters():\n",
    "        p.data -= 0.0005 * p.grad \n",
    "\n",
    "    # print runtime performance updates\n",
    "    num_prints = 10 if epochs >= 10 else epochs\n",
    "    if (k % (epochs // num_prints) == 0 or k == epochs-1):\n",
    "        print(f\"step {k}, loss: {total_loss.data[0]:.5f}, accuracy: {acc[0][0]*100:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
